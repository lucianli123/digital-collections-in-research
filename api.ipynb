{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879b7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b247087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "913739a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84040f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def strip(s):\n",
    "    s=' '.join(s.split())\n",
    "    return re.sub(r'[^A-Za-z0-9 ]+', '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43985773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b448c19",
   "metadata": {},
   "source": [
    "DHQ scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e64d02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://www.digitalhumanities.org/dhq/index/title.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2108c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucia\\miniconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(requests.get(url).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ebe54a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "links=[]\n",
    "for para in soup.find_all(\"a\"):\n",
    "    count+=1\n",
    "    if count<92:\n",
    "        continue\n",
    "    link=para['href']\n",
    "    if link[0]==\"/\":\n",
    "        links.append(\"http://www.digitalhumanities.org/\"+link)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ef603ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(requests.get(\"http://www.digitalhumanities.org/dhq/vol/16/4/000643/000643.html\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eb615eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=[]\n",
    "for l in links:\n",
    "    text=\"\"\n",
    "    soup = BeautifulSoup(requests.get(l).text)\n",
    "    try:\n",
    "        date=soup.find_all(\"a\")[66].get_text()\n",
    "    except:\n",
    "        continue\n",
    "    title=strip(soup.find_all(\"h1\")[0].get_text())\n",
    "\n",
    "    authors=[]\n",
    "    for para in soup.find_all(\"a\"):\n",
    "        try:\n",
    "            if \"../bios.html\" in para[\"href\"]:\n",
    "                authors.append(strip(para.text))\n",
    "        except:\n",
    "            continue\n",
    "    for para in soup.find_all(\"div\",  {\"class\": \"ptext\"}):\n",
    "        text+=para.get_text()\n",
    "        text+=\" \"\n",
    "    try:\n",
    "        abstract=strip(soup.find_all(\"div\", {\"id\": \"abstract\"})[0].get_text()[9:])\n",
    "    except:\n",
    "        abstract=\"\"\n",
    "    out.append([date,title, authors,abstract,strip(text), l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7c20dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(out)\n",
    "df.columns=[\"date\", \"title\",\"author\", \"abstract\", \"text\", \"url\"]\n",
    "df.to_csv(\"dhq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c480b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(i, src):\n",
    "    date=i[\"published\"]\n",
    "    title=i[\"title\"]\n",
    "    authors=i[\"authors\"]\n",
    "    abstract=strip(i['summary'])\n",
    "    url = i[\"links\"][-1][\"href\"]\n",
    "    tag=i[\"tags\"]\n",
    "    request = requests.get(url)\n",
    "    filestream = io.BytesIO(request.content)\n",
    "    pdf = fitz.open(stream=filestream, filetype=\"pdf\")\n",
    "    text = \"\"\n",
    "    for page in pdf:\n",
    "        text += page.get_text()\n",
    "    return [date,title,tag, authors,abstract,strip(text),i[\"id\"], src]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76e32967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(path):\n",
    "    text=\"\"\n",
    "    doc = fitz.open(path, filetype=\"pdf\")\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "03d54008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(url):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        request = requests.get(url)\n",
    "        filestream = io.BytesIO(request.content)\n",
    "        pdf = fitz.open(stream=filestream, filetype=\"pdf\")\n",
    "        for page in pdf:\n",
    "            text += page.get_text()\n",
    "    except:\n",
    "        soup = BeautifulSoup(requests.get(url).text)\n",
    "        [s.extract() for s in soup(['header',\"footer\",'style', 'script', '[document]', 'head', 'title'])]\n",
    "        visible_text = soup.getText()\n",
    "        for para in soup.find_all(\"p\"):\n",
    "            text+=para.get_text()\n",
    "            text+=\" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be607b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"open_source.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "93d3f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download based on keyword matches\n",
    "urls=[]\n",
    "urls.append(('https://doaj.org/api/search/articles/%22google%20books%22?pageSize=100', \"goog\"))\n",
    "urls.append(('https://doaj.org/api/search/articles/%22google%20books%20ngram%20%22?pageSize=100', \"goog\"))\n",
    "urls.append(('https://doaj.org/api/search/articles/%22google%20books%20n-gram%20%22?pageSize=100', \"goog\"))\n",
    "urls.append(('https://doaj.org/api/search/articles/%22google%20ngram%20%22?pageSize=100', \"goog\"))\n",
    "urls.append(('https://doaj.org/api/search/articles/%22google%20n-gram%20%22?pageSize=100', \"goog\"))\n",
    "\n",
    "urls.append(('https://doaj.org/api/search/articles/%22hathitrust%22?pageSize=100', \"hathi\"))\n",
    "urls.append(('https://doaj.org/api/search/articles/%22hathi%20trust%22?pageSize=100', \"hathi\"))\n",
    "\n",
    "urls.append(('https://doaj.org/api/search/articles/%22internet%20archive%22?pageSize=100', \"ia\"))\n",
    "\n",
    "urls.append(('https://doaj.org/api/search/articles/%22project%20gutenberg%22?pageSize=100', \"guten\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "a2d99bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download all Journal of Cultural Analytics\n",
    "urls=[]\n",
    "urls.append((\"https://doaj.org/api/search/articles/bibjson.journal.title.exact%3A%22Journal%20of%20Cultural%20Analytics%22?pageSize=100\", \"jca\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "2cab823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=[]\n",
    "while urls:\n",
    "    dat=urls.pop(0)\n",
    "    url=dat[0]\n",
    "    mat=dat[1]\n",
    "    response = requests.get(url)\n",
    "    if \"next\" in response.json():\n",
    "        urls.append((response.json()[\"next\"],mat))\n",
    "    jr = response.json()[\"results\"]\n",
    "    for i in jr:\n",
    "        date=i[\"created_date\"]\n",
    "        title=i[\"bibjson\"][\"title\"]\n",
    "        tag=i[\"bibjson\"][\"subject\"]\n",
    "        authors=i[\"bibjson\"][\"author\"]\n",
    "        try:\n",
    "            abstract=i[\"bibjson\"][\"abstract\"]\n",
    "        except:\n",
    "            abstract=\"\"\n",
    "        journal=i[\"bibjson\"][\"journal\"][\"title\"]\n",
    "        url=i[\"bibjson\"][\"link\"][0][\"url\"]\n",
    "        try:\n",
    "            text=parse(url)\n",
    "        except:\n",
    "            text=\"\"\n",
    "        ret.append([date,title,tag, authors,abstract,strip(text),url, mat, journal])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "cda77f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(ret)\n",
    "df.columns=[\"date\", \"title\",\"tag\",\"author\", \"abstract\", \"text\", \"url\", \"match\", \"journal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "e3eacf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates(subset=[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b1ed24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing=df[df[\"text\"]==\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c0a3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"open_source.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35cd270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f466e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = feedparser.parse('http://export.arxiv.org/api/query?search_query=all:%22internet+archive%22&max_results=10000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a678410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(i, src):\n",
    "    date=i[\"published\"]\n",
    "    title=i[\"title\"]\n",
    "    authors=i[\"authors\"]\n",
    "    abstract=strip(i['summary'])\n",
    "    url = i[\"links\"][-1][\"href\"]\n",
    "    tag=i[\"tags\"]\n",
    "    request = requests.get(url)\n",
    "    filestream = io.BytesIO(request.content)\n",
    "    pdf = fitz.open(stream=filestream, filetype=\"pdf\")\n",
    "    text = \"\"\n",
    "    for page in pdf:\n",
    "        text += page.get_text()\n",
    "    return [date,title,tag, authors,abstract,strip(text),i[\"id\"], src]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9735a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "ents=[]\n",
    "feed = feedparser.parse('http://export.arxiv.org/api/query?search_query=all:hathitrust&max_results=10000')\n",
    "for i in feed.entries:\n",
    "    ents.append(annotate(i, \"hathi\"))\n",
    "feed = feedparser.parse('http://export.arxiv.org/api/query?search_query=all:%22internet+archive%22&max_results=10000')\n",
    "for i in feed.entries:\n",
    "    ents.append(annotate(i, \"ia\"))\n",
    "feed = feedparser.parse('http://export.arxiv.org/api/query?search_query=all:%22hathi+trust%22&max_results=10000')\n",
    "for i in feed.entries:\n",
    "    ents.append(annotate(i, \"hathi\"))\n",
    "feed = feedparser.parse('http://export.arxiv.org/api/query?search_query=all:%22google+books%22&max_results=10000')\n",
    "for i in feed.entries:\n",
    "    ents.append(annotate(i, \"goog\"))\n",
    "feed = feedparser.parse('http://export.arxiv.org/api/query?search_query=all:%22google+ngram%22&max_results=10000')\n",
    "for i in feed.entries:\n",
    "    ents.append(annotate(i, \"goog\"))\n",
    "feed = feedparser.parse('http://export.arxiv.org/api/query?search_query=all:%22google+books+ngram%22&max_results=10000')\n",
    "for i in feed.entries:\n",
    "    ents.append(annotate(i, \"goog\"))\n",
    "feed = feedparser.parse('http://export.arxiv.org/api/query?search_query=all:%22project+gutenberg%22&max_results=10000')\n",
    "for i in feed.entries:\n",
    "    ents.append(annotate(i, \"guten\"))\n",
    "feed = feedparser.parse('http://export.arxiv.org/api/query?search_query=all:%22google+n-gram%22&max_results=10000')\n",
    "for i in feed.entries:\n",
    "    ents.append(annotate(i, \"goog\"))\n",
    "feed = feedparser.parse('http://export.arxiv.org/api/query?search_query=all:%22google+books+n-gram%22&max_results=10000')\n",
    "for i in feed.entries:\n",
    "    ents.append(annotate(i, \"goog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8d60ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(ents)\n",
    "df.columns=[\"date\", \"title\",\"tag\",\"author\", \"abstract\", \"text\", \"url\", \"match\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9ef1e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"arxiv.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
